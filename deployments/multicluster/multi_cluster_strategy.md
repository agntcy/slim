# Multi-Cluster Deployment Strategy

## Description

The multi-cluster deployment strategy demonstrates how SLIM nodes in different Kubernetes clusters can be connected to each other, enabling clients connecting to any cluster to reach each other seamlessly. This strategy is specifically designed for distributed environments where workloads span multiple clusters or cloud regions.

**Target Audience:**
- Multi-cluster environments requiring cross-cluster communication
- Distributed applications spanning multiple Kubernetes clusters
- Organizations with geographically distributed infrastructure
- Disaster recovery and high availability scenarios across clusters

**Use Cases:**
- Cross-cluster service mesh communication
- Multi-region application deployments
- Disaster recovery with active-active clusters
- Hybrid cloud deployments
- Edge computing scenarios with central coordination

## Details

SLIM nodes are deployed as StatefulSet on two separate clusters (cluster-a and cluster-b), each exposing an external LoadBalancer endpoint. The Controller is deployed on cluster-a and its endpoint is exposed via LoadBalancer for cross-cluster access. 

Two clusters are federated using SPIRE Federation, which enables secure cross-cluster identity and authentication. Routes are created by the Controller upon new subscriptions, allowing SLIM nodes to connect to each other across cluster boundaries using mutual TLS (mTLS) with certificates generated by SPIRE Federation.

**Key Features:**
- Cross-cluster SLIM node connectivity
- SPIRE Federation for secure cross-cluster authentication
- Centralized Controller deployment with cross-cluster access
- External endpoint configuration for inter-cluster communication
- Automatic route management across clusters

The deployment uses LoadBalancer services to expose SLIM endpoints externally, enabling cross-cluster connectivity. Each SLIM node is configured with both local and external endpoints to support intra-cluster and inter-cluster communication patterns.

![SLIM Multi-Cluster Deployment Diagram](img/slim_multicluster.svg)

## Setup Steps in Detail

Follow these steps to deploy SLIM using the multi-cluster deployment strategy:

### 1. Set up the Kubernetes clusters
```
sudo task multi-cluster:up
```

<details>
  <summary>More Details on Cluster Setup</summary>
  
  This step creates two Kind clusters:
  - `slim-cluster-a` - Primary cluster hosting the Controller
  - `slim-cluster-b` - Secondary cluster for distributed workloads
  
  Kind Cloud Provider is used for LoadBalancer support.
  
  Each cluster gets its own kubeconfig context for management.
</details>

### 2. Deploy Spire (federated mode enabled) on cluster-a & cluster-b
```
task spire:deploy
```

<details>
  <summary>More Details on Federated Spire</summary>
  
  This step deploys SPIRE Server, Agent and Controller on both clusters with federation capabilities enabled. Unlike single-cluster deployments, federated SPIRE allows cross-cluster identity verification and certificate exchange.
  
  Key federation configurations:
  - Each cluster has its own trust domain (`cluster-a.example`, `cluster-b.example`)
  - SPIRE servers are configured to federate with each other
  - Cross-cluster certificate bundles are automatically exchanged
  - JWT-SVIDs can be verified across trust domains
  
  The federation enables:
  ```
  spire:
    enabled: true
    trustedDomains:
    - spiffe://cluster-b.example  # In cluster-a configuration
  ```
  
  Useful commands for troubleshooting federation:
  ```
  # Check federation status on cluster-a
  kubectl exec -n spire spire-server-0 -- /opt/spire/bin/spire-server federation show
  
  # List federated bundle endpoints
  kubectl exec -n spire spire-server-0 -- /opt/spire/bin/spire-server federation list
  ```
</details>

### 3. Add spire endpoints to /etc/hosts
```
task slim:wait-for-lb CONTROL_HOSTNAME=spire.cluster-a.example
task slim:wait-for-lb CONTROL_HOSTNAME=spire.cluster-b.example
```

<details>
  <summary>More Details</summary>
  
  Since Kind clusters use LoadBalancer services, you need to add the assigned IP addresses to your local `/etc/hosts` file for proper DNS resolution. The wait-for-lb task will:
  - Wait for LoadBalancer IP assignment
  - Display the IP address for manual addition to `/etc/hosts`
  
  Example `/etc/hosts` entries:
  ```
  172.18.255.200   spire.cluster-a.example
  172.18.255.201   spire.cluster-b.example
  ```
</details>

### 4. Create cluster federation resources on both clusters
```
task spire:federation:deploy
```

<details>
  <summary>More Details on Federation Resources</summary>
  
  This step creates the necessary Kubernetes resources to establish SPIRE federation between clusters:
  
  - **FederationRelationship** resources defining trust relationships
  - **ClusterFederatedTrustDomain** resources for cross-cluster trust
  - Network policies allowing federation traffic
  - Service accounts and RBAC for federation endpoints
  
  The federation resources enable:
  - Automatic certificate bundle exchange between clusters
  - Cross-cluster workload identity verification
  - Federated JWT-SVID validation
  
  Example federation configuration:
  ```
  apiVersion: spire.spiffe.io/v1alpha1
  kind: ClusterFederatedTrustDomain
  metadata:
    name: cluster-b-example
  spec:
    trustDomain: cluster-b.example
    bundleEndpointURL: https://spire.cluster-b.example:8443
  ```
</details>

### 5. Deploy controller on cluster-a
```
task slim:controller:deploy
```

<details>
  <summary>More Details on Multi-Cluster Controller</summary>
  
  The Controller is deployed only on cluster-a but configured to accept connections from both clusters. Key multi-cluster configurations:
  
  ```
  config:
    southbound:
      httpHost: 0.0.0.0
      httpPort: "50052"
      tls:
        useSpiffe: true
      spire:
        socketPath: "unix:///run/spire/agent-sockets/api.sock"
  
  spire:
    enabled: true
    trustedDomains:
    - spiffe://cluster-b.example  # Trust cluster-b workloads
  
  service:
    type: LoadBalancer  # External access for cross-cluster
  ```
  
  The Controller handles:
  - Route management across both clusters
  - Cross-cluster subscription routing
  - Federated identity verification for SLIM nodes
  - Load balancing of cross-cluster connections
</details>

### 6. Add controller endpoint to /etc/hosts
```
task slim:wait-for-lb CONTROL_HOSTNAME=slim-control.cluster-a.example
```

### 7. Deploy SLIM on cluster-a & cluster-b
```
task slim:deploy
```

<details>
  <summary>More Details on Multi-Cluster SLIM Deployment</summary>
  
  SLIM is deployed on both clusters with specific multi-cluster configurations. Key differences from single-cluster deployment:
  
  **External Endpoint Configuration:**
  ```
  dataplane:
    servers:
      - endpoint: "0.0.0.0:46357"
        metadata:
          local_endpoint: ${env:MY_POD_IP}
          external_endpoint: "slim.cluster-a.example:46357"  # External LoadBalancer
        tls:
          insecure_skip_verify: false
          cert_file: "/svids/tls.crt"
          key_file: "/svids/tls.key"
          ca_file: "/svids/svid_bundle.pem"
  ```
  
  **Cross-Cluster Controller Connection:**
  ```
  controller:
    clients:
      - endpoint: "https://slim-control:50052"  # Local service name in cluster-a
      # OR
      - endpoint: "https://slim-control.cluster-a.example:50052"  # External endpoint from cluster-b
  ```
  
  **Group Name for Cluster Identification:**
  ```
  services:
    slim/0:
      node_id: ${env:SLIM_SVC_ID}
      group_name: "cluster-a"  # Identifies which cluster this node belongs to
  ```
  
  **Federated Trust Configuration:**
  ```
  spire:
    enabled: true
    trustedDomains:
    - spiffe://cluster-b.example  # Trust the other cluster's workloads
  ```
</details>

### 8. Add SLIM endpoints to /etc/hosts
```
task slim:wait-for-lb CONTROL_HOSTNAME=slim.cluster-a.example
task slim:wait-for-lb CONTROL_HOSTNAME=slim.cluster-b.example
```

### 9. Verify the multi-cluster deployment
```
# Check SLIM pods on cluster-a
kubectl config use-context kind-slim-cluster-a
kubectl get statefulsets -n slim
kubectl get pods -n slim

# Check SLIM pods on cluster-b
kubectl config use-context kind-slim-cluster-b
kubectl get statefulsets -n slim
kubectl get pods -n slim

# Verify cross-cluster connectivity
kubectl config use-context kind-slim-cluster-a
kubectl logs -n slim slim-0 | grep "Connected to controller"
```

### 10. View SLIM logs
```
# Cluster-a logs
kubectl config use-context kind-slim-cluster-a
task slim:show-logs

# Cluster-b logs
kubectl config use-context kind-slim-cluster-b
task slim:show-logs
```

### 11. Deploy sample client applications for cross-cluster testing
```
# Deploy receiver (Alice) on cluster-a
kubectl config use-context kind-slim-cluster-a
kubectl apply -f ../client_apps/with_spire/alice-pod.yaml

# Deploy sender (Bob) on cluster-b
kubectl config use-context kind-slim-cluster-b
kubectl apply -f ../client_apps/with_spire/bob-pod.yaml
```

<details>
  <summary>More Details on Cross-Cluster Client Testing</summary>
  
  The sample applications demonstrate cross-cluster communication:
  
  - **Alice (Receiver)** on cluster-a subscribes to messages
  - **Bob (Sender)** on cluster-b publishes messages
  - Messages flow: Bob → SLIM(cluster-b) → Controller(cluster-a) → SLIM(cluster-a) → Alice
  
  Each client uses SPIRE Federation for authentication:
  ```
  containers:
  - name: alice
    env:
    - name: SPIFFE_ENDPOINT_SOCKET
      value: unix:///run/spire/agent-sockets/api.sock
    volumeMounts:
    - name: spiffe-workload-api
      mountPath: /run/spire/agent-sockets
      readOnly: true
  ```
  
  The Controller automatically creates routes when Alice subscribes, enabling Bob's messages to reach Alice across clusters.
</details>

### 12. Commands to check the deployment

```
# Check federation status
kubectl config use-context kind-slim-cluster-a
kubectl exec -n spire spire-server-0 -- /opt/spire/bin/spire-server federation show

# Check SLIM node registrations on controller
kubectl config use-context kind-slim-cluster-a
kubectl logs -n slim deployment/slim-control | grep "Node registered"

# Check cross-cluster routes
kubectl config use-context kind-slim-cluster-a
kubectl logs -n slim deployment/slim-control | grep "Route added"

# Test client connectivity
kubectl config use-context kind-slim-cluster-a
kubectl logs alice

kubectl config use-context kind-slim-cluster-b
kubectl logs bob
```

### 13. Clean up when done
```
# Delete SLIM from both clusters
kubectl config use-context kind-slim-cluster-a
task slim:delete

kubectl config use-context kind-slim-cluster-b
task slim:delete

# Delete clusters
task multi-cluster:down
```

## Key Differences from Single-Cluster Deployment

### Configuration Differences

**1. External Endpoint Configuration:**
- Single-cluster: Uses internal service names only
- Multi-cluster: Requires `external_endpoint` metadata for cross-cluster access

**2. Trust Domain Configuration:**
- Single-cluster: No `trustedDomains` needed
- Multi-cluster: Must specify `trustedDomains` for each federated cluster

**3. Service Types:**
- Single-cluster: Can use `ClusterIP` services
- Multi-cluster: Requires `LoadBalancer` services for external access

**4. Group Names:**
- Single-cluster: Optional group identification
- Multi-cluster: Required `group_name` for cluster identification

**5. Controller Deployment:**
- Single-cluster: Controller can be deployed anywhere
- Multi-cluster: Controller typically on primary cluster with external access

### Federation Requirements

The multi-cluster deployment requires additional federation components:
- SPIRE Federation configured between clusters
- Cross-cluster trust domain configuration
- External LoadBalancer services for inter-cluster communication
- DNS resolution for external endpoints (via /etc/hosts or DNS server)

**Note:** The multi-cluster strategy uses cluster-specific values files (`cluster-a-values.yaml`, `cluster-b-values.yaml`) that contain the federation and external endpoint configurations necessary for cross-cluster communication. Review and customize these files according to your specific cluster networking and security requirements.